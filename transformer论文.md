##                                                摘要

显性序列转导模型基于复杂的循环或卷积神经网络，其中包括编码器和解码器。表现最好的模型还通过**注意机制**连接编码器和解码器。我们提出了一种新的简单网络架构Transformer，它完全基于注意机制，完全摒弃了递归和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优越，同时更具并行性，并且需要的训练时间明显更少。我们的模型在WMT 2014英德翻译任务上实现了28.4 BLEU，比现有的最佳结果(包括集合)提高了2个以上的BLEU。关于WMT 2014英法翻译任务。在8个gpu上训练3.5天后，我们的模型建立了一个新的单模型最先进的BLEU分数为41.8，这是文献中最佳模型训练成本的一小部分。我们通过将Transformer成功地应用于具有大量和有限训练数据的英语选区解析，证明了它可以很好地推广到其他任务。
### 1介绍

递归神经网络，特别是**长短期记忆**和**门控递归神经网络**，它们已经被牢固地确立为序列建模和语言建模和机器翻译等转导问题[35]。2. 51. 此后，大量的努力继续推动循环语言模型和编码器-解码器架构的边界[38,24,15]。

（       *同等贡献。前后顺序随机。Jakob提出用self-attention取代rnn，并开始努力评估这个想法。阿希什和伊利亚设计并实现了第一批transformer模型，并在这项工作的各个方面都发挥了重要作用。诺姆提出了缩放的点积注意、多头注意和无参数的噪声表示，他是另一个参与几乎均匀细节的人。Niki在我们的原始代码库中设计、实现、调整和评估了无数的模型变体，tensor2tensor Tlion也对新的模型变体进行了实验，这对我们的初始码期和有效的推理和可视化是非常重要的。Lukasz和Aidan花了无数个漫长的日子来设计和实现tensor2tensor的各个部分，取代了我们早期的代码库，极大地改善了结果，极大地加速了我们的研究。）

递归模型通常沿输入和输出序列的符号位置进行因子计算。将位置与计算时间中的步骤对齐，它们生成隐藏状态ht序列，作为前一个隐藏状态ht-1的函数以及位置t的输入。这种固有的顺序性质排除了训练示例内的并行化，这在较长的序列长度下变得至关重要，因为内存约束限制了跨示例的批处理。最近的工作已经通过**因数分解技巧[21]**和**条件计算[32]**实现了计算效率的显著提高，同时也提高了模型在后者情况下的性能。然而，顺序计算的基本约束仍然存在。

注意机制已经成为各种任务中引人注目的序列建模和转导模型的一个组成部分。允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离[2,19]。然而，在除少数情况外的所有情况下[27]，这种注意机制将被用于与循环网络结合使用。

在这项工作中，我们提出了Transformer，这是一种避开递归的模型架构，而是完全依赖于注意机制来绘制输入和输出之间的全局依赖关系。l Transformer允许更多的并行化，并且在8个P100 gpu上训练12个小时后，就可以在1中提高翻译质量达到一个新的状态了。

### 2背景 ###

减少顺序计算的目标也构成了**Extended Neural GPu[16]**、**ByteNet[18]**和**ConvS2S[9]**的基础，它们都使用卷积神经网络作为基本构建块，对所有输入和输出位置并行计算隐藏表示。在这些模型中，将来自两个任意输入或输出位置的信号关联起来所需的操作数量随着位置之间的距离而增长。ConyS2S为线性，ByteNet为对数。这使得学习距离较远位置之间的依赖关系变得更加困难[12]。在Transformer中，这被减少到一个恒定数量的操作，尽管其代价是降低了平均注意力加权位置的有效分辨率演进（resolution），我们用第3.2节中描述的多头注意抵消了这一影响。

自我注意，有时被称为内注意，是一种与单个序列的不同位置相关的注意机制，以计算序列的表示。self-attention已经成功地应用于各种任务中，包括阅读理解、抽象总结、文本蕴涵和学习任务独立的**句子表征[4,27,28,22]**。

端到端记忆网络基于循环注意机制，而不是顺序排列的递归，并已被证明在简单语言问答和语言建模任务中表现良好[34]。

然而，据我们所知，Transformer是第一个完全依赖于self-attention来计算其输入和输出表示的转导模型，而不使用序列对齐的rnn或卷积。在接下来的章节中，我们将描述Transformer，激发self-attention，并讨论它相对于[17,18]和[9]等模型的优势。

### 3模型体系结构 ###

大多数竞争性的神经序列转导模型都具有**编码器-解码器结构[5,2,35]**。在这里，编码器映射符号表示的输入序列(x1，…，xn)映射到一个连续表示序列z = (z1,..., zn)。给定z，解码器然后生成符号的输出序列(y1,...yu)，每次一个元素。在每一步中，模型都是**自动回归的[10]**（auto-regressive），在生成下一个元素时，将之前生成的符号作为额外输入消耗掉。
Transformer遵循这个整体架构，使用堆叠的self-attention和点向的，编码器和解码器的完全连接层，分别在图1的左半部分和右半部分中显示。



![论文1](C:\Users\Lenovo\Desktop\py\NLP\论文1.png)

output probably 输出概率
Softmax  最大池
linear  线性
Add & Norm  添加&规范
feed forward  向前传播
muti-headAttention  多头注意
Positional encoding 位置编码
input/output Embedding 输入、输出嵌入
                                                                    图1:transformer——模型架构。  

#### 3.1编码器和解码器堆栈

**编码器:**编码器由N = 6个相同层的堆栈组成。每一层有两个子layer。第一层是多头self-attention机制，第二层是简单的位置全连接前馈网络。我们使用**残差连接**[11]围绕每个这样的layer进行layer规范化，即每个这样的layer的输出是LayerNorm(x + sublayer(x))，其中sublayer(x)是由子layer本身实现的函数。为了方便这些残余连接，模型中的所有子层，以及嵌入的解码器:解码器也由N = 6个相同的layer堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头关注。类似于编码器，我们在每个子层周围采用残差连接，然后进行层归一化。我们还修改了解码器堆栈中的self-attention子层，以防止位置关注后续位置。Thisl
层，产生维度dmodel=512的输出。

**解码器：**解码器也是由N=6个相同层的堆栈组成的。除了这两个子层在每个编码器层中，解码器插入第三个子层，执行多头注意编码器堆栈的输出。与编码器类似，我们使用残差连接围绕每个子层，然后进行层规范化。我们也会改变自我注意解码器堆栈中的子层，以防止位置关注后续位置。这屏蔽，再加上输出嵌入偏移一个位置的事实，确保了对位置i的预测可以仅取决于在小于i的位置处的已知输出。

#### 3.2注意力

注意函数可以被描述为将查询和一组键值对映射到输出，其中查询的键值和输出值都是向量，输出被计算为加权和
的值，其中分配给每个值的权重是由的兼容性函数计算的查询对应的键。

Scaled Dot-Product Attention ：缩放的点积关注
Multi-Head Attention：多头的关注

Concat ：
SoftMax ：

![论文2](C:\Users\Lenovo\Desktop\py\NLP\论文2.png)

​                               图2:(左)缩放点积注意力。(右)多头注意由并行运行的几个注意层组成。

##### 3.2.1缩放点积关注

我们称我们的特殊关注为“缩放点积关注”(图2)。输入由维度dk的所有查询和键以及维度d的值组成。我们用所有键计算查询的点积，每个点积除以Vdk，并应用softmax函数来获得值的权重。在实践中，我们同时计算一组查询的注意力函数，并将其打包在一起。
键和值也打包到矩阵K和v中，我们计算输出的矩阵为:
![论文3](C:\Users\Lenovo\Desktop\py\NLP\论文3.png)        （1）
两种最常用的注意函数是加性注意[2]和点积(乘)注意。点积注意与我们的算法是相同的，除了的缩放因子。加性注意使用具有单个隐藏layer的前馈网络来计算兼容性函数。而两者在理论复杂度上是相似的。在实践中，点积注意要快得多，而且更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。
虽然对于较小的dk值，这两种机制的表现相似，但在不缩放较大dk值的情况下，加性注意力优于点积注意力[3]。我们怀疑，对于较大的dk值，点积的大小会变大，从而将softmax函数推入它已经存在的区域极小的梯度。为了抵消这种影响，我们将点积缩放为1 /√dk

##### 3.2.2多头注意力

我们发现，与其使用dmodel维的键、值和查询执行单一的注意函数，不如将查询、键和值分别以不同的、学习过的线性投影h次线性投影到dk、dk和dv维度，这是有益的。然后，在l查询、键和值的每一个投影版本上，我们并行地执行注意函数，产生d维输出值。这些被连接并再次投影，产生最终值，如图2所示。
(为了说明为什么点积会变大，假设q和k的分量是独立的随机变量，均值为0，方差为1。那么它们的点积q·k = 所有qiki相加的均值为O，方差为dk。)
多头关注允许模型在不同位置共同关注来自不同表示子空间的信息。对于单个注意头，平均会抑制这一点。
![论文4](C:\Users\Lenovo\Desktop\py\NLP\论文4.png)





其中投影是参数矩阵WQi在 Rdmodel×dk（也就是模型迪卡尔积集合的空间）之中, WKi, WVi，WOi同理
在这项工作中，我们使用h= 8个平行的注意层，（或者说头）。对于每一个，我们使用dk = dv= dmodel/h= 64。由于每个头像的维数降低，总计算成本与全维的单头注意相似。

##### 3.2.3注意力在我们模型中的应用

transformer以三种不同的方式使用多头注意力:
~在“编码器-解码器注意力”层中，查询来自前一个解码器层，并且存储器键和值来自编码器的输出。这允许解码器       中的位置，以关注输入序列中的所有位置。这模仿了序列到序列模型中的典型编码器-解码器注意机制。

~编码器包含自我注意层。在自我关注层中，所有的键、值和查询来自同一个地方，在这种情况下，是来自编码器。编码器中的每个位置都可以处理上一层中的所有位置编码器。

~类似地，解码器中的自关注层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防 止左倾解码器中的信息流以保持自回归特性。我们通过屏蔽（设置为-∞）输入中的所有值来引起缩放点积的内部注意来实施这一点。对应于非法连接的softmax的。见图

#### 3.3位置前馈网络

除了注意子层之外，编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络以sentically和相同的方式与每个位置相关联，这由两个线性变换组成，中间有一个ReLU激活。
                                                  FFN(x) = max(0, xWi +b1)W2+b2                                                   （2）
虽然线性变换在不同位置上是相同的，但它们每层使用不同的参数。另一种描述这种情况的方法是两个核大小为1的卷积。输入和输出的维数为dmodel = 512，内层的维数dff = 2048。

#### 3.4嵌入和Softmax

与其他序列转导模型类似，我们使用学习的嵌入将输入代参（tokens）和输出代参（tokens）转换为维数为dmodel的向量。我们还使用通常学习到的线性变换和softmax函数将解码器输出转换为预测的下一个代参（tokens）概率。在我们的模型中，我们在两个emhedding layers和nre-sottmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以根号dmodel

#### 3.5位置编码

由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于序列的相对或绝对位置的信息



表1:最大路径长度，每层复杂度和最小顺序操作次数针对不同的层类型。N为序列长度，d为表示维数，k为核卷积的大小，r是受限self-attention中邻域的大小。

![屏幕截图 2023-05-21 192552](C:\Users\Lenovo\Desktop\py\NLP\屏幕截图 2023-05-21 192552.png)

Layer Type：层类型
 Complexity per Layer：每层的复杂度 

Sequential Operations顺序操作
Maximum Path Length：最大路径长度

Self-Attention：自注意
Recurrent ：复发性
Convolutional：卷积
Self-Attention(限制)

以上是序列中的token。为此，我们在输入嵌入中添加了“位置编码”编码器和解码器堆栈的底部。位置编码与嵌入具有相同的维数模型，因此两者可以相加。位置编码有很多选择，有学习的和固定的[9]。
在这项工作中，我们使用了不同频率的正弦和余弦函数:
                          PE（pos，2i） = sin（pos/10000的2i/dmadel次方）
                          PE（pos, 21 +1) = cos(pos/ 10000的2i/dmadel次方)
其中pos是位置，I是尺寸。也就是说，位置编码的每一个维对应一个正弦波。波长形成从2n到10000 2n的几何级数。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地通过相对位置来学习，因为对于任何固定的偏移量k. PEnos+k都可以表示为PEpos的线性函数

我们还尝试使用学习的位置嵌入[9]，并发现两个版本引入了几乎相同的结果(参见Tahle 3行(E))。我们选择正弦版本，因为它可能允许模型外推到比训练期间遇到的序列长度更长的序列长度。

### 4为什么要自我注意

在本节中，我们将self-attention层的各个方面与通常用于将符号表示的可变长度序列sl (x1...... xn)映射到另一个具有zi, xi (属于Rd的向量空间) 的等长度序列(21,zn)的循环和卷积layers进行比较，例如典型序列转导编码器或解码器中的隐藏层。激发我们对self-attention的使用，我们要考虑三个需要考虑的因素。

一个是每层的总计算复杂度。另一个是可以并行化的计算量，以所需的最小顺序操作数来衡量。三是网络中远程依赖关系之间的路径长度。学习远程依赖关系是许多序列转导任务中的关键挑战。影响学习这种依赖关系的能力的一个关键因素是网络中向前和向后信号必须穿越的路径的长度。输入和输出序列中任意位置组合之间的这些路径越短，学习远程依赖关系就越容易[121]。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，self-attention层用恒定数量的顺序执行操作连接所有位置，而循环layer需要O(n)个顺序操作。就计算复杂度而言，当序列长度n小于表示维数d时，self-attention层比循环层更快，这是机器翻译中最先进的模型使用的句子表示最常见的情况，例如词块[38]和字节对[31]表示。以提高涉及的任务的计算性能的非常长的序列，自我注意可能仅限于考虑大小为r的邻域输入序列以各自的输出位置为中心。这将使最大路径长度增加到O(n/r)。我们计划在未来的工作中进一步研究这种方法。

核宽度k <n的单个卷积层不会连接所有对输入和输出位置。在核相邻的情况下，这样做需要一个O(n/k)个卷积layers的堆栈。或者在扩展卷积的情况下需要O(logk(n))[18]，增加网络中任意两个位置之间最长路径的长度。卷积层通常比循环层要昂贵k倍。然而，可分离卷积[6]大大降低了复杂性，为O(k n d+ n d²)。然而，即使k = n，可分离卷积的复杂性也等于self-attention层和点向前馈层的组合，这是我们在模型中采用的方法。

作为附带好处，self-attention可以产生更多可解释的模型。我们从我们的模型中检查注意力分布，并在附录中提供和讨论示例。不仅个人注意力头清楚地学会执行不同的任务，许多人似乎表现出与句法相关的行为以及句子的语义结构。

### 5训练

本节描述了我们模型的训练机制。
#### 5.1训练数据和批处理

我们在标准的WMT 2014英语-德语数据集上进行训练，该数据集包含大约450万个句子对。句子使用字节对编码进行编码[3]，该编码具有大约37000个标记的共享源- 1目标词汇表。对于英语-法语，我们使用了更大的WMT 2014英语-法语数据集，该数据集由36M个句子组成，并将token拆分为32000个词片词汇[38]。通过近似序列长度将句子对批处理在一起。每个训练批包含一组包含大约25000个源标记和25000个目标标记的句子对。

#### 5.2硬件和时间表

我们在一台带有8个NVIDIA P100 gpu的机器上训练我们的模型。对于使用本文中描述的超参数的基本模型，每个训练步骤大约需要0.4秒。对于我们的大模型，我们总共训练了100.00个样本或2个小时的基本模型。(如表3底线所述)，步长时间为1.0秒。大模型训练了30万步。(3.5天)。

#### 5.3优化器

我们使用了Adam优化器[20]，B1=0.9, B2=0.98, e= 10e-9。我们在训练过程中改变学习率，根据公式:

![屏幕截图 2023-05-21 193633](C:\Users\Lenovo\Desktop\py\NLP\屏幕截图 2023-05-21 193633.png)



这对应于在第一个warmup_steps训练步骤中线性增加学习率，然后按步数的倒数平方根成比例地降低学习率。我们使用了*warmup_steps* = 4000。

#### 5.4正规化（图片）

我们在训练过程中使用了三种类型的正则化:
**残留Dropout**      我们将Dropout[33]应用于每个子层的输出，然后将其添加到子层输入并归一化。此外，我们将dropout应用于编码器和解码器堆栈中的嵌入和位置编码的总和。对于基本模型，我们使用的速率为
Idrop = 0.1

![屏幕截图 2023-05-23 161626](C:\Users\Lenovo\Desktop\py\NLP\屏幕截图 2023-05-23 161626.png)

表2:Transformer在英语-德语和英语-法语newstest2014测试中取得了比以前的最先进型号更好的BLEU分数，而培训成本只是其中的一小部分。

字节网络
GNMT+RL [38]
ConvS2S [9]
MOE[32]
Deep-att +PosUnk Ensemble [39]
GNMT + RL Ensemble38]
ConvS2S Ensemble[9]
Transformer(基本型号)
Transformer(大)

在训练过程中，我们采用值els = 0.1的标签平滑[36]。这令人困惑，因为模型学习更不确定，但提高准确性和BLEU分数。

### 6结果

#### 6.1机器翻译

关于WMT 2014英译德翻译任务，大变压器模型(transformer(大))表2中)的性能比之前报道的最佳模型(包括ensembles)高出2.0lBLEU以上。建立了一个新的最先进的BLEU分数28.4。这个模型的配置是列在表3的底线。训练用了3.5天，使用了8个P100 gpu。即我们的基础模型超越了所有以前发表的模型和集合，在任何竞争模型的一小部分的培训成本。在WMT 2014英语到法语翻译任务上，我们的大模型达到了41.0的BLEU分数，优于之前发布的所有单一模型，而训练成本不到之前最先进模型的1/4。英语到法语训练的Transformer(大)模型使用的辍学率Pdrop = 0.1，而不是0.3。对于基础模型，我们使用了通过平均最后5个检查点获得的单个模型，这些检查点每隔10分钟编写一次。对于大模型，我们取最后20个检查点的平均值。我们使用波束搜索，波束大小为4，长度惩罚0 = 0.6[38]。这些超参数是在开发集上实验后选择的。我们将推理过程中的最大输出长度设置为输入长度+50，但在可能的情况下尽早终止[38]。表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过将训练时间、使用的GPU数量和每个GPU的持续单精度浮点容量的估计值5相乘来估计用于训练模型的浮点运算次数。

#### 6.2模型变化（图片）

为了评估变压器不同组件的重要性，我们以不同的方式改变了我们的基本模型，测量了在开发集上英语到德语翻译的性能变化，newstest2013。我们使用了前一节中描述的波束搜索，但没有使用检查点平均。我们将这些结果呈现在表3中。在表3行(A)中，我们改变了注意头的数量以及注意键和值维度，保持计算量不变，如3.2.2节所述。虽然单头注意力比最佳设置差0.9 BLEU，但过多的头也会降低质量。我们对K80、K40、M40和P100分别使用了2.8、3.7、6.0和9.5 TFLOPS的值。

表3:Transformer体系结构的变化。未列出的值与基本模型的值相同。所有的指标都是在英语到德语的翻译开发集，newstest2013。根据我们的字节对编码，列出的困惑是每个单词的，不应该与每个单词的困惑进行比较。



![屏幕截图 2023-05-23 163129](C:\Users\Lenovo\Desktop\py\NLP\屏幕截图 2023-05-23 163129.png)



表4:Transformer可以很好地推广到英语选区解析(结果见第23节)
《华尔街日报》)

![屏幕截图 2023-05-23 163217](C:\Users\Lenovo\Desktop\py\NLP\屏幕截图 2023-05-23 163217.png)

在表3行(B)中，我们观察到减小注意键大小d会损害模型质量。这这表明确定兼容性并不容易，更复杂的兼容性函数比点积可能更有益。我们在(C)和(D)行中进一步观察到，正如预期的那样，更大的模型更好，dropout对于避免过度拟合非常有帮助。在(E)行中，我们替换我们的正弦位置编码与学习的位置嵌入[9]，观察到几乎相同的结果与基本模型。

#### 6.3英语选区解析

为了评估Transformer是否可以推广到其他任务，我们对英语进行了实验选区解析。这项任务提出了具体的挑战:输出受制于强结构性约束，并且明显长于输入。此外，RNN序列到序列模型还不能在小数据体系中获得最先进的结果[37]。我们在《华尔街日报》(WSJ)部分训练了一个dmodel=1024的4层变压器Penn Treebank[25]，大约40K的训练句子。我们也在半监督的环境下训练它，使用大约17M个句子的更大的高置信度和BerkleyParser语料库[37]。我们仅在WSJ设置中使用了16K代币的词汇表，并使用了32K代币的词汇表对于半监督设置。我们只进行了少量的实验来选择dropout，包括注意力和残差(第5.4节)，在section 22开发集上的学习率和光束大小，所有其他参数从英语到德语的基本翻译模式保持不变。在推理过程中，我们将最大输出长度增加到输入长度+ 300。我们使用的光束尺寸为21,o = 0.3适用于WSJ和半监督环境。我们在表4中的结果显示，尽管缺乏针对特定任务的调优，我们的模型表现出奇地好，除了递归神经网络语法(Recurrent Neural Network Grammar)[8]外，它的结果比之前报道的所有模型都要好。与RNN序列到序列模型[37]相比，即使仅在40K句子的WSJ训练集上训练，Transformer也优于BerkeleyParser[29]。注意，用多头self-attention取代编码器-解码器架构中最常用的循环层。
### 7结论

在这项工作中，我们提出了Transformer，这是第一个完全基于的序列转导模型
对于翻译任务，Transformer的训练速度明显快于基于循环层或卷积层的架构。关于WMT 2014英语到德语和WMT 2014英法翻译任务，达到了我们的新境界。在前一项任务中我们做到了最好模型的表现甚至超过了之前报道的所有组合。我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将Transformer扩展到涉及文本和以外的输入和输出模式的问题研究局部的、受限的注意力机制，以有效地处理大量的输入和输出
比如图像、音频和视频。让生成不那么顺序化是我们的另一个研究目标。
tensorflow / tensor2tensor。
我们用来训练和评估模型的代码可以在https://github.com/上找到

### EP ###

注意力可视化

![屏幕截图 2023-05-23 163735](C:\Users\Lenovo\Desktop\py\NLP\屏幕截图 2023-05-23 163735.png)

​         图3：在

编码器在第5层（共6层）中的自我注意。多头注意力集中关注的是动词“make”，完成短语“make…more difficult”。此处显示的注意事项仅适用于“make”这个词。不同的颜色代表不同的头部。彩色观看最佳。
